---
title: "Activity_1"
output: html_document
date: "2024-07-31"
---

```{r setup, include=FALSE}
library(dplyr)
library(ggplot2)
library(tidyr)
library(car)
library(MASS)
library(corrplot)
library(reshape2)
library(faraway)
library(caret)
```

## 1. Import and clean data

```{r}
data <- read.csv("auto_mpg.csv", header = TRUE, sep = ";")
attach(data)
head(data)
dim(data)
```

```{r}
# Replace "?" with NA in the horsepower column
data$horsepower[data$horsepower == "?"] <- NA
```

```{r}
# Convert horsepower to numeric (it was read as a factor due to "?")
data$horsepower <- as.numeric(as.character(data$horsepower))
```

```{r}

# Check for missing values
missing_values <- sapply(data, function(x) sum(is.na(x)))
print(missing_values)
```

```{r}
# Handle missing values in the 'horsepower' column

# Option 1: Remove rows with missing values
data_clean <- na.omit(data)
# Reasons:
# - The 'horsepower' column has only 6 missing values, so removing them will not affect the dataset significantly.
# - It avoids the risk of introducing bias or inaccuracies that might occur when estimating missing values.

# Option 2: Replace missing values with the mean (or median) of the column
# data$horsepower[is.na(data$horsepower)] <- mean(data$horsepower, na.rm = TRUE)
```

```{r}
# remove car_name column in data_clean
data_clean <- dplyr::select(data_clean, -car_name)
str(data_clean)
```


```{r}
data_clean %>%
  gather(key = "variable", value = "value") %>%
  ggplot(aes(x = value)) +
  facet_wrap(~ variable, scales = "free") +
  geom_histogram(bins = 30) +
  theme_minimal()
data_clean %>%
  gather(key = "variable", value = "value") %>%
  ggplot(aes(x = variable, y = value)) +
  geom_boxplot() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

As the data has almost no outliers, then we don't need to process them.

## 2. Split data to train and test
```{r}
set.seed(1)
sample_size <- floor(0.8 * nrow(data_clean))
train_indices <- sample(seq_len(nrow(data_clean)), size = sample_size)
data_train <- data_clean[train_indices, ]
data_test <- data_clean[-train_indices, ]
```

## 3. Build model
### Check multicolinearity
```{r}
model1 <- lm(mgp ~. , data = data_train)
summary(model1)
```
```{r}
vif(model1)
```
We consider correlation matrix
```{r}
cor_matrix <- cor(data_train)
cor_matrix
```
heat map:
```{r}
corrplot(cor_matrix, method = "color", type = "upper", 
         tl.col = "black", tl.srt = 45, addCoef.col = "black")

# Convert the correlation matrix to long format for ggplot2
cor_matrix_melted <- melt(cor_matrix)

# Visualize with ggplot2
ggplot(data = cor_matrix_melted, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), space = "Lab", 
                       name = "Correlation") +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                   size = 12, hjust = 1)) +
  coord_fixed()
```
We can see that vif of displacement is high (21.407632). Then, we will remove it from the model.

```{r}
model1 <- lm(mgp ~ . -displacement, data = data_train)
summary(model1)
```
```{r}
vif(model1)
```


We can see that vif of weight variable is the highest (8.4456). We would first consider remove weight variable.

```{r}
model1 <- lm(mgp ~ . -displacement -weight, data = data_train)
summary(model1)
```
After removing weight variable, we see that the adjusted R-squared decreases significantly. Thus, we try removing the second highest vif which is horsepower variable.

```{r}
model1 <- lm(mgp ~ . -displacement -horsepower, data = data_train)
summary(model1)
```

```{r}
vif(model1)
```
removing cylinders

```{r}
model1 <- lm(mgp ~ . -displacement -horsepower -cylinders, data = data_train)
summary(model1)
```
```{r}
vif(model1)
```
### Variable selection
```{r}
modFull = lm(mgp ~ ., data = data_train)
modZero = lm(mgp ~ 1, data = data_train)
modInter = lm(mgp ~ weight + model_year, data = data_train)
```

```{r}
model2 = MASS::stepAIC(modInter, direction = "both", scope = list(lower = modZero, upper = modFull), k = 2)
```

```{r}
model3 = MASS::stepAIC(modInter, direction = "both", scope = list(lower = modZero, upper = modFull), k = log(nrow(data_train)))
```

## 4. Hypothesis testing
```{r}
summary(model2)
summary(model3)
anova(model2, model3)
```
blabla, choose model3

### Test for normality
```{r}
# Use Shapiro-Wilk test to test for normality of the residuals
shapiro.test(residuals(model3))

# Plot residuals to visually check for normality
par(mfrow=c(1,2))

hist_residuals <- residuals(model3)
hist(hist_residuals, main = "Residuals Histogram", xlab = "Residuals", breaks = 100, probability = TRUE)

mean_residuals <- mean(hist_residuals)
sd_residuals <- sd(hist_residuals)
curve(dnorm(x, mean = mean_residuals, sd = sd_residuals), col = "red", add = TRUE)

qqnorm(hist_residuals, main = "Q-Q Plot of Residuals")
qqline(hist_residuals, col = "red")
```

### Box-Cox
```{r}
boxcox_result <- boxcox(model3, plotit = TRUE)
lambda <- boxcox_result$x
log_likelihood <- boxcox_result$y

# Find the lambda with the maximum log-likelihood
best_lambda <- lambda[which.max(log_likelihood)]

# Print the best lambda
print(best_lambda)
```
```{r}
best_lambda <- -0.2
```

```{r}
model_cox = lm((((mgp^best_lambda) - 1)/best_lambda) ~ weight + model_year + origin)
summary(model_cox)
```

### Test for normality
```{r}
# Use Shapiro-Wilk test to test for normality of the residuals
shapiro.test(residuals(model_cox))

# Plot residuals to visually check for normality
par(mfrow=c(1,2))

hist_residuals <- residuals(model_cox)
hist(hist_residuals, main = "Residuals Histogram", xlab = "Residuals", breaks = 100, probability = TRUE)

mean_residuals <- mean(hist_residuals)
sd_residuals <- sd(hist_residuals)
curve(dnorm(x, mean = mean_residuals, sd = sd_residuals), col = "red", add = TRUE)

qqnorm(hist_residuals, main = "Q-Q Plot of Residuals")
qqline(hist_residuals, col = "red")
```
### 6. Prediction
```{r}
predictions <- predict(model_cox, newdata = data_test, type = "response")
# # Metrics
# actuals <- (data_test$mgp^best_lambda - 1) / best_lambda
# mae <- mean(abs(predictions - actuals))
# mse <- mean((predictions - actuals)^2)
# rmse <- sqrt(mse)
# print(c("MAE" = mae, "MSE" = mse, "RMSE" = rmse))
# 
# res <- (rmse) / (max(data_test$mgp) - min(data_test$mgp))
# res
```

```{r}
# summary(model_cox)
```

## 7. Evaluation
Using cross validation k-folds
```{r}
set.seed(1)
# Define a train control with k-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation

# Define the formula for the model
formula <- as.formula(paste0("((mgp^", best_lambda, " - 1)/", best_lambda, ") ~ weight + model_year + origin"))

# Train the model using the training data
cv_model <- train(formula, data = data_train, method = "lm", trControl = train_control)

# Print the summary of the cross-validated model
print(cv_model)
```
```{r}
# Predict using the cross-validated model on the test data
predictions <- predict(cv_model, newdata = data_test)

# Calculate performance metrics on the test data
actual_values <- (data_test$mgp^best_lambda - 1) / best_lambda
results <- data.frame(
  Actual = actual_values,
  Predicted = predictions
)

# Print the results
# print(results)

# Calculate and print RMSE and R-squared
rmse <- sqrt(mean((results$Actual - results$Predicted)^2))
r_squared <- cor(results$Actual, results$Predicted)^2

cat("RMSE:", rmse, "\n")
cat("R-squared:", r_squared, "\n")
```

