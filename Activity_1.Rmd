---
title: "Activity_1"
output: html_document
---

```{r setup, include=FALSE}
library(dplyr)
library(ggplot2)
library(tidyr)
library(car)
library(MASS)
library(corrplot)
library(reshape2)
library(faraway)
library(caret)
library(lmtest)
```

## 1. Import and clean data

```{r}
data <- read.csv("auto_mpg.csv", header = TRUE, sep = ";")
attach(data)
head(data)
dim(data)
```

```{r}
# Replace "?" with NA in the horsepower column
data$horsepower[data$horsepower == "?"] <- NA
```

```{r}
# Convert horsepower to numeric (it was read as a factor due to "?")
data$horsepower <- as.numeric(as.character(data$horsepower))
```

```{r}
# Check for missing values
missing_values <- sapply(data, function(x) sum(is.na(x)))
print(missing_values)
```

```{r}
# Handle missing values in the 'horsepower' column

# Option 1: Remove rows with missing values
data_clean <- na.omit(data)
# Reasons:
# - The 'horsepower' column has only 6 missing values, so removing them will not affect the dataset significantly.
# - It avoids the risk of introducing bias or inaccuracies that might occur when estimating missing values.

# Option 2: Replace missing values with the mean (or median) of the column
# data$horsepower[is.na(data$horsepower)] <- mean(data$horsepower, na.rm = TRUE)
```

```{r}
# Check duplicate
# Identify duplicate rows based on all columns
duplicates <- duplicated(data_clean)
print(data_clean[duplicates, ])
```
There is no duplicate value.

```{r}
# remove car_name column in data_clean
data_clean <- dplyr::select(data_clean, -car_name)
str(data_clean)
detach(data)
attach(data_clean)
```

### Descriptive statistic
```{r}
# Visualize the dataset
data_clean %>%
  gather(key = "variable", value = "value") %>%
  ggplot(aes(x = value)) +
  facet_wrap(~ variable, scales = "free") +
  geom_histogram(bins = 30) +
  theme_minimal()
data_clean %>%
  gather(key = "variable", value = "value") %>%
  ggplot(aes(x = variable, y = value)) +
  geom_boxplot() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

As the data has almost no outliers, then we don't need to process them.

```{r}
summary(data_clean)
```

```{r}
pie(table(origin))
barplot(table(origin))
```
```{r}
pie(table(model_year))
```
```{r}
pie(table(cylinders))
barplot(table(cylinders))
```

The product from North American accounted for the highest percentage while the others (Europe, Asia) accounted for equally percentage.

model_year distribution is approximately distributed equally through years.

```{r}
hist(mgp, breaks = 50)
```
The mgp distribution is right skewed. Hence, we process this by taking log(mgp)^2

```{r}
log_mgp_square = (log10(data_clean$mgp))^2
data_clean$log_mgp_square <- log_mgp_square
hist(log_mgp_square, breaks = 25)
```


```{r}
boxplot(log_mgp_square ~ origin)
```
The highest mgp consumption coming from Asia countries while the lowest one coming from North America. It indicates that the engines from North America might be better than the other areas.

```{r}
boxplot(log_mgp_square ~ model_year)
```
There is a rising trend of mgp through out the period from 1970 to 1982.

```{r}
boxplot(log_mgp_square ~ cylinders)
boxplot(horsepower ~ cylinders, data = data_clean)
```
Overall, engines having more cylinders seems to be stronger while consuming less fuel.

```{r}
boxplot(displacement ~ cylinders)
```
The engines having more cylinders seem to have larger displacement.

```{r}
plot(log_mgp_square ~ horsepower)
plot(log_mgp_square ~ displacement)
plot(log_mgp_square ~ weight)
plot(log_mgp_square ~ acceleration)
```
Overall, there is a correlation between mgp and horsepower, displacement, weight while there is no correlation between mgp and acceleration.

### Create dummy variable
```{r}
min(model_year)
max(model_year)
# Convert year from 1970-1982 to 1-13
data_clean$model_year <- data_clean$model_year - 1970 + 1
```


```{r}
# Create dummy variables for 'origin' column
data_clean$north_american <- ifelse(data_clean$origin == 1, 1, 0)
data_clean$europe <- ifelse(data_clean$origin == 2, 1, 0)
data_clean$origin <- NULL
```

```{r}
str(data_clean)
```

## 2. Split data to train and test
```{r}
set.seed(1)
sample_size <- floor(0.8 * nrow(data_clean))
train_indices <- sample(seq_len(nrow(data_clean)), size = sample_size)
data_train <- data_clean[train_indices, ]
data_test <- data_clean[-train_indices, ]
detach(data_clean)
attach(data_train)
```

## 3. Build model
### Check multicolinearity
```{r}
model1 <- lm(log_mgp_square ~. -mgp, data = data_train)
summary(model1)
```

```{r}
car::vif(model1)
```

We consider correlation matrix
```{r}
cor_matrix <- cor(data_train)
cor_matrix
```

Heat map:
```{r}
corrplot(cor_matrix, method = "color", type = "upper", 
         tl.col = "black", tl.srt = 45, addCoef.col = "black")

# Convert the correlation matrix to long format for ggplot2
cor_matrix_melted <- melt(cor_matrix)

# Visualize with ggplot2
ggplot(data = cor_matrix_melted, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), space = "Lab", 
                       name = "Correlation") +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                   size = 12, hjust = 1)) +
  coord_fixed()
```
After reviewing the correlation matrix and calculating VIF values for each predictor, we observe that the 'displacement' variable has the highest VIF value (22.109484). A high VIF value indicates a strong correlation with other predictor variables, suggesting multicollinearity, which can inflate the variance of coefficient estimates and make the model unstable.

```{r}
model_displacement <- lm(displacement ~ . -mgp -log_mgp_square, data = data_train)
summary(model_displacement)
```
The R-squared = 0.9548. Moreover, there are strong correlation among displacement towards cylinders, horsepower, weight, acceleration, and north_american.

To address multicollinearity and improve the model's stability, we will remove the 'displacement' variable from the model.

```{r}
model1 <- lm(log_mgp_square ~ . -mgp -displacement, data = data_train)
summary(model1)
```

```{r}
car::vif(model1)
```


We observe that 'weight' variable has the highest VIF value this time (8.462281). We try removing 'weight' variable and see if that helps.

```{r}
model1 <- lm(log_mgp_square ~ . -mgp -displacement -weight, data = data_train)
summary(model1)
```

After removing 'weight' variable, the adjusted R-squared decreases significantly from 0.8606 to 0.8173. Thus, we try removing predictor having the second highest VIF value (8.198353) which is 'horsepower' variable.

```{r}
model1 <- lm(log_mgp_square ~ . -mgp -displacement -horsepower, data = data_train)
summary(model1)
```

The result of removing 'horsepower' variable is actually way better than the one after removing 'weight' variable through the decrease of residual standard error from 0.1696 to 0.1486 and the climb of adjusted R-squared from 0.8173 to 0.8598.

```{r}
model_horsepower <- lm(horsepower ~ . -mgp -log_mgp_square -displacement, data = data_train)
summary(model_horsepower)
```
The R-squared = 0.878. Moreover, there are strong correlation among horsepower towards cylinders, weight, acceleration, model_year, and north_american.

```{r}
car::vif(model1)
```

Since VIF value of 'cylinders' 5.692103, we try removing cylinders to hope for a better result: 

```{r}
model1 <- lm(log_mgp_square ~ . -mgp -displacement -horsepower -cylinders, data = data_train)
summary(model1)
```
Before removing 'cylinders' variable, the adjusted R-squared = 0.8598. Then, after removing 'cylinders' variable, the model R-squared climbs significantly to 0.8602. Hence, we consider removing this variable.

```{r}
model_cylinders <- lm(cylinders ~ . -mgp -log_mgp_square -displacement -horsepower, data = data_train)
summary(model_cylinders)
```
The R-squared = 0.8287. Moreover, there are strong correlation among cylinders towards weight, acceleration, and north_american.

Therefore, we decide to remove 'cylinders' predictor variable due to collinearity.

```{r}
car::vif(model1)
```
The vif values of remaining predictor are less than 5. Hence, there is unlikely collinearity among variables.

### Variable selection
```{r}
modFull = lm(log_mgp_square ~ . -mgp -displacement -horsepower -cylinders, data = data_train)
modZero = lm(log_mgp_square ~ 1, data = data_train)
modInter = lm(log_mgp_square ~ weight + model_year, data = data_train)
```

```{r}
model2 = MASS::stepAIC(modInter, direction = "both", scope = list(lower = modZero, upper = modFull), k = 2)
```

```{r}
model3 = MASS::stepAIC(modInter, direction = "both", scope = list(lower = modZero, upper = modFull), k = log(nrow(data_train)))
```

## 4. Hypothesis testing
```{r}
summary(model2)
summary(model3)
anova(model2, model3)
```
-p_value >< correlation
-R-square change
-plot
=> remove acceleration => model3

### Shapiro-Wilk test for normality
```{r}
# Use Shapiro-Wilk test to test for normality of the residuals
shapiro.test(residuals(model3))

# Plot residuals to visually check for normality
par(mfrow=c(1,2))

hist_residuals <- residuals(model3)
hist(hist_residuals, main = "Residuals Histogram", xlab = "Residuals", breaks = 30, probability = TRUE)

mean_residuals <- mean(hist_residuals)
sd_residuals <- sd(hist_residuals)
curve(dnorm(x, mean = mean_residuals, sd = sd_residuals), col = "red", add = TRUE)

qqnorm(hist_residuals, main = "Q-Q Plot of Residuals")
qqline(hist_residuals, col = "red")
```

### Studentized Breusch-Pagan test for heteroscedasticity
```{r}
bp_test <- bptest(model3)
print(bp_test)
```

### Box-Cox Transformation
```{r}
boxcox_result <- boxcox(model3, plotit = TRUE)
lambda <- boxcox_result$x
log_likelihood <- boxcox_result$y

# Find the lambda with the maximum log-likelihood
best_lambda <- lambda[which.max(log_likelihood)]

# Print the best lambda
print(best_lambda)
```


```{r}
# Build the final model with Box-Cox transformation
best_lambda = 0.5
model_cox = lm((((data_train$log_mgp_square^best_lambda) - 1)/best_lambda) ~ data_train$weight + data_train$model_year + data_train$north_american)
summary(model_cox)
```
```{r}
# Check normality of residuals
residuals <- residuals(model_cox)

## Shapiro-Wilk test for residuals normality
shapiro_test <- shapiro.test(residuals)
print(shapiro_test)

## Q-Q plot
qqnorm(residuals)
qqline(residuals, col = "red")

## Plot residuals to visually check for normality
par(mfrow=c(1,2))

hist_residuals <- residuals(model_cox)
hist(hist_residuals, main = "Residuals Histogram", xlab = "Residuals", breaks = 30, probability = TRUE)

mean_residuals <- mean(hist_residuals)
sd_residuals <- sd(hist_residuals)
curve(dnorm(x, mean = mean_residuals, sd = sd_residuals), col = "red", add = TRUE)
```
```{r}
# Breusch-Pagan test to check for heteroscedasticity (constant variance of residuals)
bp_test <- bptest(model_cox)
print(bp_test)
```


## 5. Interpretation
```{r}
summary(model_cox)
```
### Quantile
- 25% of residuals are less than -0.06215  
- 50% of residuals are above 0.00484, 50% of residuals are below 0.00484  
- 75% of residuals are less than 0.06370  

### Coefficent of predictors
Estimated Regression model: $\hat{\frac{(log\_mgp\_square)^{\lambda_{best}} - 1}{\lambda_{best}}} = \hat\beta_0 + \hat\beta_1 \cdot weight + \hat \beta_2 \cdot model\_year + \hat\beta_3 \cdot north\_american$

- $\hat\beta_0$: $\hat{\frac{(log\_mgp\_square)^{\lambda_{best}} - 1}{\lambda_{best}}} =  5.588e+01$ when weight = 0, model_year = 0, north_american = 0

- $\hat\beta_1$: For each unit increase in weight, on average, the expected value of $\hat{\frac{(log\_mgp\_square)^{\lambda_{best}} - 1}{\lambda_{best}}$ decreases by 2.498e-04, holding other predictors constant.

- $\hat\beta_2$: For each unit increase in model_year, on average, the expected value of $\hat{\frac{(log\_mgp\_square)^{\lambda_{best}} - 1}{\lambda_{best}}$ increases by 2.874e-02, holding other predictors constant.

- $\hat\beta_3$: if the engine is in north_american, on average, the expected value of $\hat{\frac{(log\_mgp\_square)^{\lambda_{best}} - 1}{\lambda_{best}}$ decreases by 6.065e-02, holding other predictors constant.

### Multiple R-squared and Adjusted R-squared
- Multiple R-squared: 0.8721 inteprets that 87.21% of the variance in the response variable $\hat{\frac{(log\_mgp\_square)^{\lambda_{best}} - 1}{\lambda_{best}}}$ can be explained by the predictor variables (weight, model_year, north_american) in the model.

### Residual standard error
- Residual standard error: rse = 0.1059 indicates that the model's predictions are, on average, approximately 0.1059 units away from the actual values of $\hat{\frac{(log\_mgp\_square)^{\lambda_{best}} - 1}{\lambda_{best}}$. Some points are further from the line than this rse, other points are closer to the line than this rse.

## 6,7. Evaluation
###Using cross validation k-folds
```{r}
set.seed(1)
# Define a train control with k-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation

# Define the formula for the model
formula <- as.formula(paste0("((log_mgp_square^", best_lambda, " - 1)/", best_lambda, ") ~ weight + model_year + north_american"))

# Train the model using the training data
cv_model <- train(formula, data = data_train, method = "lm", trControl = train_control)

# Print the summary of the cross-validated model
print(cv_model)
```
```{r}
# Predict using the cross-validated model on the test data
predictions <- predict(cv_model, newdata = data_test)

# Calculate performance metrics on the test data
actual_values <- (data_test$log_mgp_square^best_lambda - 1) / best_lambda
results <- data.frame(
  Actual = actual_values,
  Predicted = predictions
)

# Print the results
print(results)

# Calculate and print RMSE and R-squared
rmse <- sqrt(mean((results$Actual - results$Predicted)^2))
r_squared <- cor(results$Actual, results$Predicted)^2
cat("RMSE:", rmse, "\n")
cat("R-squared:", r_squared, "\n")
```