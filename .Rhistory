str(data_clean)
data_clean %>%
gather(key = "variable", value = "value") %>%
ggplot(aes(x = value)) +
facet_wrap(~ variable, scales = "free") +
geom_histogram(bins = 30) +
theme_minimal()
data_clean %>%
gather(key = "variable", value = "value") %>%
ggplot(aes(x = variable, y = value)) +
geom_boxplot() +
theme_minimal() +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
set.seed(1)
sample_size <- floor(0.8 * nrow(data_clean))
train_indices <- sample(seq_len(nrow(data_clean)), size = sample_size)
data_train <- data_clean[train_indices, ]
data_test <- data_clean[-train_indices, ]
model1 <- lm(mgp ~. , data = data_train)
summary(model1)
vif(model1)
cor_matrix <- cor(data_train)
cor_matrix
model1 <- lm(mgp ~ . -displacement, data = data_train)
summary(model1)
vif(model1)
cor(mgp, weight)
model1 <- lm(mgp ~ . -displacement -horsepower, data = data_train)
summary(model1)
vif(model1)
model1 <- lm(mgp ~ . -displacement -horsepower -cylinders, data = data_train)
summary(model1)
vif(model1)
corrplot(cor_matrix, method = "color", type = "upper",
tl.col = "black", tl.srt = 45, addCoef.col = "black")
library(dplyr)
library(ggplot2)
library(tidyr)
library(car)
library(MASS)
library(corrplot)
library(reshape2)
install.packages("reshape2")
library(dplyr)
library(ggplot2)
library(tidyr)
library(car)
library(MASS)
library(corrplot)
library(reshape2)
corrplot(cor_matrix, method = "color", type = "upper",
tl.col = "black", tl.srt = 45, addCoef.col = "black")
# Convert the correlation matrix to long format for ggplot2
cor_matrix_melted <- melt(cor_matrix)
# Visualize with ggplot2
ggplot(data = cor_matrix_melted, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1, 1), space = "Lab",
name = "Correlation") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1,
size = 12, hjust = 1)) +
coord_fixed()
model1 <- lm(mgp ~ . -displacement, data = data_train)
summary(model1)
vif(model1)
model1 <- lm(mgp ~ . -displacement, data = data_train)
summary(model1)
vif(model1)
model1 <- lm(mgp ~ . -displacement -horsepower, data = data_train)
summary(model1)
model1 <- lm(mgp ~ . -displacement -weight, data = data_train)
summary(model1)
model1 <- lm(mgp ~ . -displacement -horsepower, data = data_train)
summary(model1)
model1 <- lm(mgp ~ . -displacement -horsepower -weight, data = data_train)
summary(model1)
model1 <- lm(mgp ~ . -displacement -horsepower -cylinders, data = data_train)
summary(model1)
modFull = lm(mgp ~ ., data = data_train)
modZero = lm(mgp ~ 1, data = data_train)
modInter = lm(mgp ~ weight + model_year, data = data_train)
model2 = MASS::stepAIC(modInter, direction = "both", scope = list(lower = modZero, upper = modFull), k = 2)
model2 = MASS::stepAIC(modInter, direction = "both", scope = list(lower = modZero, upper = modFull), k = log(nrow(data_train)))
model2 = MASS::stepAIC(modInter, direction = "both", scope = list(lower = modZero, upper = modFull), k = 2)
model3 = MASS::stepAIC(modInter, direction = "both", scope = list(lower = modZero, upper = modFull), k = log(nrow(data_train)))
summary(model2)
summary(model3)
summary(model2)
summary(model3)
anova(model2, model3)
# Use Shapiro-Wilk test to test for normality of the residuals
shapiro.test(residuals(model3))
# Plot residuals to visually check for normality
par(mfrow=c(1,2))
hist_residuals <- residuals(model3)
hist(hist_residuals, main = "Residuals Histogram", xlab = "Residuals", breaks = 100, probability = TRUE)
mean_residuals <- mean(hist_residuals)
sd_residuals <- sd(hist_residuals)
curve(dnorm(x, mean = mean_residuals, sd = sd_residuals), col = "red", add = TRUE)
qqnorm(hist_residuals, main = "Q-Q Plot of Residuals")
qqline(hist_residuals, col = "red")
library(dplyr)
library(ggplot2)
library(tidyr)
library(car)
library(MASS)
library(corrplot)
library(reshape2)
library(faraway)
install.packages("faraway")
library(dplyr)
library(ggplot2)
library(tidyr)
library(car)
library(MASS)
library(corrplot)
library(reshape2)
library(faraway)
boxcox(model_4, plotit = TRUE)
boxcox(model3, plotit = TRUE)
boxcox_result <- boxcox(model3, plotit = TRUE)
lambda <- boxcox_result$x
log_likelihood <- boxcox_result$y
# Find the lambda with the maximum log-likelihood
best_lambda <- lambda[which.max(log_likelihood)]
# Print the best lambda
print(best_lambda)
best_lambda <- -0.25
model_cox = lm((((mgp^best_lambda) - 1)/best_lambda) ~ weight + model_year + origin)
summary(model_cox)
# Use Shapiro-Wilk test to test for normality of the residuals
shapiro.test(residuals(model3))
# Plot residuals to visually check for normality
par(mfrow=c(1,2))
hist_residuals <- residuals(model3)
hist(hist_residuals, main = "Residuals Histogram", xlab = "Residuals", breaks = 100, probability = TRUE)
mean_residuals <- mean(hist_residuals)
sd_residuals <- sd(hist_residuals)
curve(dnorm(x, mean = mean_residuals, sd = sd_residuals), col = "red", add = TRUE)
qqnorm(hist_residuals, main = "Q-Q Plot of Residuals")
qqline(hist_residuals, col = "red")
# Use Shapiro-Wilk test to test for normality of the residuals
shapiro.test(residuals(model_cox))
# Plot residuals to visually check for normality
par(mfrow=c(1,2))
hist_residuals <- residuals(model_cox)
hist(hist_residuals, main = "Residuals Histogram", xlab = "Residuals", breaks = 100, probability = TRUE)
mean_residuals <- mean(hist_residuals)
sd_residuals <- sd(hist_residuals)
curve(dnorm(x, mean = mean_residuals, sd = sd_residuals), col = "red", add = TRUE)
qqnorm(hist_residuals, main = "Q-Q Plot of Residuals")
qqline(hist_residuals, col = "red")
boxcox_result <- boxcox(model3, plotit = TRUE)
lambda <- boxcox_result$x
log_likelihood <- boxcox_result$y
# Find the lambda with the maximum log-likelihood
best_lambda <- lambda[which.max(log_likelihood)]
# Print the best lambda
print(best_lambda)
model_cox = lm((((mgp^best_lambda) - 1)/best_lambda) ~ weight + model_year + origin)
summary(model_cox)
# Use Shapiro-Wilk test to test for normality of the residuals
shapiro.test(residuals(model_cox))
# Plot residuals to visually check for normality
par(mfrow=c(1,2))
hist_residuals <- residuals(model_cox)
hist(hist_residuals, main = "Residuals Histogram", xlab = "Residuals", breaks = 100, probability = TRUE)
mean_residuals <- mean(hist_residuals)
sd_residuals <- sd(hist_residuals)
curve(dnorm(x, mean = mean_residuals, sd = sd_residuals), col = "red", add = TRUE)
qqnorm(hist_residuals, main = "Q-Q Plot of Residuals")
qqline(hist_residuals, col = "red")
best_lambda <- -0.25
model_cox = lm((((mgp^best_lambda) - 1)/best_lambda) ~ weight + model_year + origin)
summary(model_cox)
# Use Shapiro-Wilk test to test for normality of the residuals
shapiro.test(residuals(model_cox))
# Plot residuals to visually check for normality
par(mfrow=c(1,2))
hist_residuals <- residuals(model_cox)
hist(hist_residuals, main = "Residuals Histogram", xlab = "Residuals", breaks = 100, probability = TRUE)
mean_residuals <- mean(hist_residuals)
sd_residuals <- sd(hist_residuals)
curve(dnorm(x, mean = mean_residuals, sd = sd_residuals), col = "red", add = TRUE)
qqnorm(hist_residuals, main = "Q-Q Plot of Residuals")
qqline(hist_residuals, col = "red")
boxcox_result <- boxcox(model3, plotit = TRUE)
lambda <- boxcox_result$x
log_likelihood <- boxcox_result$y
# Find the lambda with the maximum log-likelihood
best_lambda <- lambda[which.max(log_likelihood)]
# Print the best lambda
print(best_lambda)
model_cox = lm((((mgp^best_lambda) - 1)/best_lambda) ~ weight + model_year + origin)
summary(model_cox)
# Use Shapiro-Wilk test to test for normality of the residuals
shapiro.test(residuals(model_cox))
# Plot residuals to visually check for normality
par(mfrow=c(1,2))
hist_residuals <- residuals(model_cox)
hist(hist_residuals, main = "Residuals Histogram", xlab = "Residuals", breaks = 100, probability = TRUE)
mean_residuals <- mean(hist_residuals)
sd_residuals <- sd(hist_residuals)
curve(dnorm(x, mean = mean_residuals, sd = sd_residuals), col = "red", add = TRUE)
qqnorm(hist_residuals, main = "Q-Q Plot of Residuals")
qqline(hist_residuals, col = "red")
best_lambda <- -0.2
model_cox = lm((((mgp^best_lambda) - 1)/best_lambda) ~ weight + model_year + origin)
summary(model_cox)
# Use Shapiro-Wilk test to test for normality of the residuals
shapiro.test(residuals(model_cox))
# Plot residuals to visually check for normality
par(mfrow=c(1,2))
hist_residuals <- residuals(model_cox)
hist(hist_residuals, main = "Residuals Histogram", xlab = "Residuals", breaks = 100, probability = TRUE)
mean_residuals <- mean(hist_residuals)
sd_residuals <- sd(hist_residuals)
curve(dnorm(x, mean = mean_residuals, sd = sd_residuals), col = "red", add = TRUE)
qqnorm(hist_residuals, main = "Q-Q Plot of Residuals")
qqline(hist_residuals, col = "red")
best_lambda <- -0.1
model_cox = lm((((mgp^best_lambda) - 1)/best_lambda) ~ weight + model_year + origin)
summary(model_cox)
# Use Shapiro-Wilk test to test for normality of the residuals
shapiro.test(residuals(model_cox))
# Plot residuals to visually check for normality
par(mfrow=c(1,2))
hist_residuals <- residuals(model_cox)
hist(hist_residuals, main = "Residuals Histogram", xlab = "Residuals", breaks = 100, probability = TRUE)
mean_residuals <- mean(hist_residuals)
sd_residuals <- sd(hist_residuals)
curve(dnorm(x, mean = mean_residuals, sd = sd_residuals), col = "red", add = TRUE)
qqnorm(hist_residuals, main = "Q-Q Plot of Residuals")
qqline(hist_residuals, col = "red")
best_lambda <- 0.1
model_cox = lm((((mgp^best_lambda) - 1)/best_lambda) ~ weight + model_year + origin)
summary(model_cox)
# Use Shapiro-Wilk test to test for normality of the residuals
shapiro.test(residuals(model_cox))
# Plot residuals to visually check for normality
par(mfrow=c(1,2))
hist_residuals <- residuals(model_cox)
hist(hist_residuals, main = "Residuals Histogram", xlab = "Residuals", breaks = 100, probability = TRUE)
mean_residuals <- mean(hist_residuals)
sd_residuals <- sd(hist_residuals)
curve(dnorm(x, mean = mean_residuals, sd = sd_residuals), col = "red", add = TRUE)
qqnorm(hist_residuals, main = "Q-Q Plot of Residuals")
qqline(hist_residuals, col = "red")
best_lambda <- 0.2
model_cox = lm((((mgp^best_lambda) - 1)/best_lambda) ~ weight + model_year + origin)
summary(model_cox)
# Use Shapiro-Wilk test to test for normality of the residuals
shapiro.test(residuals(model_cox))
# Plot residuals to visually check for normality
par(mfrow=c(1,2))
hist_residuals <- residuals(model_cox)
hist(hist_residuals, main = "Residuals Histogram", xlab = "Residuals", breaks = 100, probability = TRUE)
mean_residuals <- mean(hist_residuals)
sd_residuals <- sd(hist_residuals)
curve(dnorm(x, mean = mean_residuals, sd = sd_residuals), col = "red", add = TRUE)
qqnorm(hist_residuals, main = "Q-Q Plot of Residuals")
qqline(hist_residuals, col = "red")
best_lambda <- 0
model_cox = lm((((mgp^best_lambda) - 1)/best_lambda) ~ weight + model_year + origin)
best_lambda <- 0.05
model_cox = lm((((mgp^best_lambda) - 1)/best_lambda) ~ weight + model_year + origin)
summary(model_cox)
# Use Shapiro-Wilk test to test for normality of the residuals
shapiro.test(residuals(model_cox))
# Plot residuals to visually check for normality
par(mfrow=c(1,2))
hist_residuals <- residuals(model_cox)
hist(hist_residuals, main = "Residuals Histogram", xlab = "Residuals", breaks = 100, probability = TRUE)
mean_residuals <- mean(hist_residuals)
sd_residuals <- sd(hist_residuals)
curve(dnorm(x, mean = mean_residuals, sd = sd_residuals), col = "red", add = TRUE)
qqnorm(hist_residuals, main = "Q-Q Plot of Residuals")
qqline(hist_residuals, col = "red")
best_lambda <- -0.2
model_cox = lm((((mgp^best_lambda) - 1)/best_lambda) ~ weight + model_year + origin)
summary(model_cox)
# Use Shapiro-Wilk test to test for normality of the residuals
shapiro.test(residuals(model_cox))
# Plot residuals to visually check for normality
par(mfrow=c(1,2))
hist_residuals <- residuals(model_cox)
hist(hist_residuals, main = "Residuals Histogram", xlab = "Residuals", breaks = 100, probability = TRUE)
mean_residuals <- mean(hist_residuals)
sd_residuals <- sd(hist_residuals)
curve(dnorm(x, mean = mean_residuals, sd = sd_residuals), col = "red", add = TRUE)
qqnorm(hist_residuals, main = "Q-Q Plot of Residuals")
qqline(hist_residuals, col = "red")
boxcox_result <- boxcox(model3, plotit = TRUE)
lambda <- boxcox_result$x
log_likelihood <- boxcox_result$y
# Find the lambda with the maximum log-likelihood
best_lambda <- lambda[which.max(log_likelihood)]
# Print the best lambda
print(best_lambda)
predictions <- predict(model_cox, newdata = data_test, type = "response")
# Metrics
actuals <- data_test$mgp
mae <- mean(abs(predictions - actuals))
mse <- mean((predictions - actuals)^2)
rmse <- sqrt(mse)
r_squared <- 1 - sum((predictions - actuals)^2) / sum((actuals - mean(actuals))^2)
rmse
predictions <- predict(model_cox, newdata = data_test, type = "response")
# Metrics
actuals <- data_test$mgp
mae <- mean(abs(predictions - actuals))
mse <- mean((predictions - actuals)^2)
rmse <- sqrt(mse)
r_squared <- 1 - sum((predictions - actuals)^2) / sum((actuals - mean(actuals))^2)
print(c("MAE" = mae, "MSE" = mse, "RMSE" = rmse, "R-squared" = r_squared))
prediction <- predict(model_cox, newdata = data_test)
actual <- data_test$mgp
mse <- mean((prediction - actual)^2)
rmse <- sqrt(mse)
rmse
predictions <- predict(model_cox, newdata = data_test, type = "response")
# Metrics
actuals <- data_test$mgp
mae <- mean(abs(predictions - actuals))
mse <- mean((predictions - actuals)^2)
rmse <- sqrt(mse)
r_squared <- 1 - sum((predictions - actuals)^2) / sum((actuals - mean(actuals))^2)
print(c("MAE" = mae, "MSE" = mse, "RMSE" = rmse, "R-squared" = r_squared))
sumary(model_cox)
summary(model_cox)
predictions <- predict(model_cox, newdata = data_test, type = "response")
# Metrics
actuals <- data_test$mgp
mae <- mean(abs(predictions - actuals))
mse <- mean((predictions - actuals)^2)
rmse <- sqrt(mse)
r_squared <- 1 - sum((predictions - actuals)^2) / sum((actuals - mean(actuals))^2)
print(c("MAE" = mae, "MSE" = mse, "RMSE" = rmse, "R-squared" = r_squared))
res <- (rmse) / (max(data_test$mgp) - min(data_test$mgp))
res
library(dplyr)
library(ggplot2)
library(tidyr)
library(car)
library(MASS)
library(corrplot)
library(reshape2)
library(faraway)
library(caret)
install.packages("caret")
library(dplyr)
library(ggplot2)
library(tidyr)
library(car)
library(MASS)
library(corrplot)
library(reshape2)
library(faraway)
library(caret)
# Define a train control with k-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
# Define the formula for the model
formula <- (((mgp^best_lambda) - 1)/best_lambda) ~ weight + model_year + origin
# Train the model using the training data
cv_model <- train(formula, data = data_train, method = "lm", trControl = train_control)
# Define a train control with k-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
# Define the formula for the model
formula <- (((mgp^best_lambda) - 1)/best_lambda) ~ weight + model_year + origin
# Train the model using the training data
cv_model <- train(formula, data = data_train, method = "lm", trControl = train_control)
# Define a train control with k-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
# Define the formula for the model
formula <- (((mgp^best_lambda) - 1)/best_lambda) ~ weight + model_year + origin
# Train the model using the training data
cv_model <- train(formula, data = data_train, method = "lm")
# Define a train control with k-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
# Define the formula for the model
fformula <- as.formula(paste0("((mgp^", best_lambda, " - 1)/", best_lambda, ") ~ weight + model_year + origin"))
# Train the model using the training data
cv_model <- train(formula, data = data_train, method = "lm", trControl = train_control)
# Define a train control with k-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
# Define the formula for the model
fformula <- as.formula(paste0("((mgp^", best_lambda, " - 1)/", best_lambda, ") ~ weight + model_year + origin"))
# Train the model using the training data
cv_model <- train(formula, data = data_train, method = "lm", trControl = train_control)
# Define a train control with k-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
# Define the formula for the model
formula <- as.formula(paste0("((mgp^", best_lambda, " - 1)/", best_lambda, ") ~ weight + model_year + origin"))
# Train the model using the training data
cv_model <- train(formula, data = data_train, method = "lm", trControl = train_control)
# Print the summary of the cross-validated model
print(cv_model)
# Predict using the cross-validated model on the test data
predictions <- predict(cv_model, newdata = data_test)
# Calculate performance metrics on the test data
actual_values <- (data_test$mgp^best_lambda - 1) / best_lambda
results <- data.frame(
Actual = actual_values,
Predicted = predictions
)
# Print the results
print(results)
# Calculate and print RMSE and R-squared
rmse <- sqrt(mean((results$Actual - results$Predicted)^2))
r_squared <- cor(results$Actual, results$Predicted)^2
cat("RMSE:", rmse, "\n")
cat("R-squared:", r_squared, "\n")
# Define a train control with k-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
# Define the formula for the model
formula <- as.formula(paste0("((mgp^", best_lambda, " - 1)/", best_lambda, ") ~ weight + model_year + origin"))
# Train the model using the training data
cv_model <- train(formula, data = data_train, method = "lm", trControl = train_control)
# Print the summary of the cross-validated model
print(cv_model)
summary(cv_model)
# Define a train control with k-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
# Define the formula for the model
formula <- as.formula(paste0("((mgp^", best_lambda, " - 1)/", best_lambda, ") ~ weight + model_year + origin"))
# Train the model using the training data
cv_model <- train(formula, data = data_train, method = "lm", trControl = train_control)
# Print the summary of the cross-validated model
print(cv_model)
# Define a train control with k-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
# Define the formula for the model
formula <- as.formula(paste0("((mgp^", best_lambda, " - 1)/", best_lambda, ") ~ weight + model_year + origin"))
# Train the model using the training data
cv_model <- train(formula, data = data_train, method = "lm", trControl = train_control)
# Print the summary of the cross-validated model
print(cv_model)
set.seed(1)
# Define a train control with k-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
# Define the formula for the model
formula <- as.formula(paste0("((mgp^", best_lambda, " - 1)/", best_lambda, ") ~ weight + model_year + origin"))
# Train the model using the training data
cv_model <- train(formula, data = data_train, method = "lm", trControl = train_control)
# Print the summary of the cross-validated model
print(cv_model)
set.seed(1)
# Define a train control with k-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
# Define the formula for the model
formula <- as.formula(paste0("((mgp^", best_lambda, " - 1)/", best_lambda, ") ~ weight + model_year + origin"))
# Train the model using the training data
cv_model <- train(formula, data = data_train, method = "lm", trControl = train_control)
# Print the summary of the cross-validated model
print(cv_model)
# Predict using the cross-validated model on the test data
predictions <- predict(cv_model, newdata = data_test)
# Calculate performance metrics on the test data
actual_values <- (data_test$mgp^best_lambda - 1) / best_lambda
results <- data.frame(
Actual = actual_values,
Predicted = predictions
)
# Print the results
print(results)
# Calculate and print RMSE and R-squared
rmse <- sqrt(mean((results$Actual - results$Predicted)^2))
r_squared <- cor(results$Actual, results$Predicted)^2
cat("RMSE:", rmse, "\n")
cat("R-squared:", r_squared, "\n")
# Predict using the cross-validated model on the test data
predictions <- predict(cv_model, newdata = data_test)
# Calculate performance metrics on the test data
actual_values <- data_test$mgp
results <- data.frame(
Actual = actual_values,
Predicted = predictions
)
# Print the results
print(results)
# Calculate and print RMSE and R-squared
rmse <- sqrt(mean((results$Actual - results$Predicted)^2))
r_squared <- cor(results$Actual, results$Predicted)^2
cat("RMSE:", rmse, "\n")
cat("R-squared:", r_squared, "\n")
# Predict using the cross-validated model on the test data
predictions <- predict(cv_model, newdata = data_test)
# Calculate performance metrics on the test data
actual_values <- (data_test$mgp^best_lambda - 1) / best_lambda
results <- data.frame(
Actual = actual_values,
Predicted = predictions
)
# Print the results
print(results)
# Calculate and print RMSE and R-squared
rmse <- sqrt(mean((results$Actual - results$Predicted)^2))
r_squared <- cor(results$Actual, results$Predicted)^2
cat("RMSE:", rmse, "\n")
cat("R-squared:", r_squared, "\n")
predictions <- predict(model_cox, newdata = data_test, type = "response")
# Metrics
actuals <- (data_test$mgp^best_lambda - 1) / best_lambda
mae <- mean(abs(predictions - actuals))
mse <- mean((predictions - actuals)^2)
rmse <- sqrt(mse)
r_squared <- 1 - sum((predictions - actuals)^2) / sum((actuals - mean(actuals))^2)
print(c("MAE" = mae, "MSE" = mse, "RMSE" = rmse, "R-squared" = r_squared))
res <- (rmse) / (max(data_test$mgp) - min(data_test$mgp))
res
